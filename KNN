The K-NN working can be explained on the basis of the below algorithm:

● Step-1: Select the number K of the neighbors
● Step-2: Calculate the Euclidean distance of K number of neighbors ● Step-3:
Take the K nearest neighbors as per the calculated Euclidean distance.
● Step-4: Among these k neighbors, count the number of the data points in each
category.
● Step-5: Assign the new data points to that category for which the number of the
neighbor is maximum.
● Step-6: Our model is ready.
# Assigning features and label variables
# First Feature
weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',
'Rainy','Sunny','Overcast','Overcast','Rainy']
# Second Feature
temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']

# Label or target varible
play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']

# Import LabelEncoder
from sklearn import preprocessing
#creating labelEncoder
le = preprocessing.LabelEncoder()
# Converting string labels into numbers.
weather_encoded=le.fit_transform(weather)
print(weather_encoded)
# converting string labels into numbers
temp_encoded=le.fit_transform(temp)
print(temp_encoded)
label=le.fit_transform(play)
#combinig weather and temp into single listof tuples
features=list(zip(weather_encoded,temp_encoded))
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)
#model = KNeighborsClassifier()
# Train the model using the training sets
model.fit(features,label)
#Predict Output
predicted1= model.predict([[0,2]]) # 0:Overcast, 2:Mild
predicted2= model.predict([[2,1]]) # 2:sunny, 1:hot
print(predicted1)
print(predicted2)
