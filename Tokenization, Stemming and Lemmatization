Tokenization, Stemming and Lemmatization
Tokenization
import nltk
nltk.download('punkt')
words = nltk.word_tokenize(sentence)
print(words)

input: wait waiting waited waits

Output
['wait', 'waiting', 'waited', 'waits']

Stemming
from nltk.stem import PorterStemmer
porter=PorterStemmer()
from nltk import sent_tokenize,word_tokenize
sentence=input("Enter a sentence:")
stem_sentence=[]
def stemsentence(sentence):
for word in words:
stem_sentence.append(porter.stem(word))
stem_sentence.append(" ")
return " ".join(stem_sentence)
x=stemsentence(sentence)
print(x)
input: wait waiting waited waits

Output
wait wait wait wait

Lemmatization
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer=WordNetLemmatizer()

sentence = "He was running and eating at same time. He has bad habit of swimming after playing
long hours in the Sun."
punctuations="?:!.,;"
sentence_words = nltk.word_tokenize(sentence)
for word in sentence_words:
if word in punctuations:
sentence_words.remove(word)

#sentence_words
print("{0:20}{1:20}".format("Word","Lemma"))
for word in sentence_words:
print ("{0:20}{1:20}".format(word,wordnet_lemmatizer.lemmatize(word)))
